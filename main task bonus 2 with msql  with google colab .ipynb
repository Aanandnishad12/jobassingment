{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPDX/sTCeLq+8aJTAw1IsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanandnishad12/jobassingment/blob/main/main%20task%20bonus%202%20with%20msql%20%20with%20google%20colab%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAM8RYkg-iQ0"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver \n",
        "from selenium.webdriver.chrome.options import Options  #importing the neccery file loaded the data with pandas and got 2 series asin and country\n",
        "from bs4 import BeautifulSoup                          \n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import re\n",
        "import json \n",
        "from datetime import datetime as dt\n",
        "chrome_options = Options()\n",
        "import mysql.connector\n",
        "\n",
        "# please ad your host and user password and db in this in order to get store the data in data base\n",
        "mydb = mysql.connector.connect(                         \n",
        "  host=\"localhost\",\n",
        "  user=\"root\",\n",
        "  password=\"Anishad@123\",\n",
        "  database = \"xyz\"\n",
        ")\n",
        "mycursor = mydb.cursor()\n",
        "mycursor.execute(\"\"\"CREATE TABLE if not exists`amazon_data` (\n",
        "  `id` int(11) NOT NULL AUTO_INCREMENT,\n",
        "  `Product Title` varchar(1000) default NULL,\n",
        "  `Product Image URL` varchar(1000) default NULL,\n",
        "  `Price of the Product` varchar(1000) default NULL,\n",
        "  `Product Details` varchar(1500) default NULL,\n",
        "  KEY `id` (`id`)\n",
        ") ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "chrome_deriver_path = r\"C:\\Users\\anand nishad\\Desktop\\driver\\chromedriver.exe\"   # path of the chrome driver where it is save your directory \n",
        "# chrome_options.add_argument(\"--headless\")\n",
        "driver = webdriver.Chrome(chrome_deriver_path,options=chrome_options)\n",
        "\n",
        "\n",
        "def remove_unwanted_values(s1):\n",
        "    return s1.replace(\" \",\"\").replace(\"\\u200f\",\"\").replace(\"\\u200e\",\"\")\n",
        "\n",
        "\n",
        "data = pd.read_csv(r\"C:\\Users\\anand nishad\\Downloads\\Amazon Scraping.csv\")    # file directory of the file of the data please change to your directory\n",
        "asin = data[\"Asin\"]\n",
        "country = data[\"country\"]                                                      \n",
        "data_list = []\n",
        "ct= 0\n",
        "for i,j in zip(asin, country):\n",
        "    if ct == 0:\n",
        "        start_time = dt.now()\n",
        "    elif(ct == 100):\n",
        "        print(dt.now()-start_time,\"---time took to complete 100 urls\")       #  getting the time after a every 100 links\n",
        "        ct = -1\n",
        "    ct+=1\n",
        "\n",
        "    start_url = f\"https://www.amazon.{j}/dp/{i}\"          # running the for loop over the link passing the country and asin in f string \n",
        "    try:\n",
        "        \n",
        "        driver.get(start_url)                             # getting the url with the selenium\n",
        "        # print(start_url)\n",
        "        # sleep(6)\n",
        "        soup = BeautifulSoup(driver.page_source,\"lxml\")   # creating thr soup of source page\n",
        "        name = soup.find(\"title\").text\n",
        "        print(name)\n",
        "        if \"404\" in name:                                 # getting all the links which show 404\n",
        "            print(\"404  \"+start_url)\n",
        "            # continue\n",
        "        # print(name)\n",
        "        s = re.split('condition=new\"> <span',str(soup))   # getting the price from the soup\n",
        "        price = re.search(r\">(.*?)</span> </a> </span>\",s[1]).group(1)\n",
        "        # print(price)\n",
        "        product_details = soup.find(\"ul\",class_=\"a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list\").find_all(\"span\",class_=\"a-list-item\")\n",
        "        emp_string = \"\"                                    \n",
        "        for i in product_details:\n",
        "            i = remove_unwanted_values(i.text)            # getting the  product details fassing it into other funtion remove_unwanted_values to filter data\n",
        "            emp_string+=\"\".join(i.split(\"\\n\"))+\",  \"\n",
        "        # print(emp_string)\n",
        "\n",
        "        ima = soup.find(\"div\",{\"id\":\"leftCol\"}).find(\"img\").get(\"data-a-dynamic-image\")\n",
        "        string_ = eval(ima)                               # getting the image link since there are more than on link \n",
        "        for i in string_:\n",
        "            print(i)\n",
        "            imag_data = i\n",
        "            break\n",
        "        data_list.append({start_url:{\"Product Title\":name,\"Product Image URL\":imag_data,\"Price of the Product\":price,\"Product Details\":emp_string}})\n",
        "        # print({start_url:{\"Product Title\":name,\"Product Image URL\":imag_data,\"Price of the Product\":price,\"Product Details\":emp_string}})\n",
        "        mycursor = mydb.cursor()\n",
        "        val=list(zip((name,),(imag_data,),(price,),(emp_string,)))\n",
        "        # print(val)\n",
        "        sql = \"\"\"insert into amazon_data(`Product Title`,`Product Image URL`, `Price of the Product`,`Product Details`) values (%s,%s,%s,%s)\"\"\" \n",
        "        mycursor.executemany(sql,val) \n",
        "        mydb.commit()\n",
        "        assert j in driver                               # append all the dictionrys in teh list and will later to convert in json\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "driver.quit()\n",
        "\n",
        "final_data = json.dumps(data_list, indent = 4)           # converting the list into json file\n",
        "with open(\"amazon_products.json\", \"w\") as outfile:\n",
        "    outfile.write(final_data)"
      ]
    }
  ]
}